---
title: Effet Jaberwocky
id: 20230321151841
type: algorithme
tags:
  - ostranénie
  - désorientation
  - incompréhension
---

L'effet Jabberwocky est un phénomène qui se produit lorsque l'on utilise l'apprentissage automatique ([[20210807224135]] Machine learning] pour traiter de grandes quantités de données. Le terme fait référence au poème de _Lewis Carroll_ "Jabberwocky", dans lequel des mots inventés sont utilisés pour créer une impression de sens sans pour autant avoir de signification précise.

Lorsqu'un algorithme de machine learning est utilisé pour traiter des données massives, il peut devenir très complexe et difficile à comprendre. Les données entrées dans le modèle peuvent également contenir des erreurs ou des biais, ce qui peut conduire à des résultats inattendus. Les chercheurs peuvent alors avoir du mal à comprendre comment le modèle fonctionne exactement et comment il a produit ses résultats.

L'effet Jabberwocky peut également se produire lorsque le modèle est utilisé pour des tâches pour lesquelles il n'a pas été spécifiquement conçu. Par exemple, un modèle conçu pour la reconnaissance d'images peut être utilisé pour générer des descriptions de ces images. Les résultats peuvent sembler corrects à première vue, mais sans une compréhension précise de la façon dont le modèle fonctionne, il peut être difficile de dire avec certitude si les résultats sont fiables ou non.

En résumé, l'effet Jabberwocky souligne la difficulté à comprendre les résultats produits par des algorithmes de machine learning très complexes et à grande échelle, en raison de la masse de données traitées et de la difficulté à comprendre le cheminement effectué et l'impossibilité de le comprendre et de le reproduire.
Le concept a été créé par Olivier Le Deuff en 2020 dans un de ces cours sur les enjeux algorithmiques autour des données.